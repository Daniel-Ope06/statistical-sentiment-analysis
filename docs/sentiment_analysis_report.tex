\documentclass[12pt, a4paper]{article}

% --- Font and Language Setup (Requires XeLaTeX compiler) ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{carlito}
\renewcommand{\familydefault}{\sfdefault}

% --- Packages ---
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm} % Standard academic margins
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}

% --- Python Code Formatting Setup ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- Document Info ---
\title{
    \Large \textbf{ITNPAI3: AI for NLP} \\
    \vspace{0.5cm}
    \LARGE \textbf{Sentiment Analysis of Twitter Data} \\
    \vspace{0.25cm}
    \large A Comparative Evaluation of Naive Bayes and Logistic Regression across Bag-of-Words and TF-IDF Representations
}
\author{Student ID: 3539054}
\date{February 26, 2026}

\begin{document}

\maketitle

\begin{abstract}
This report explores how to build a machine learning model capable of automatically classifying the sentiment of tweets as Positive, Negative, or Neutral. Before training the models, the raw text was cleaned and standardised. This included a custom "negation handling" step, which links words like "not" to the word that follows it (for example, turning "not good" into "not\_good") so the model correctly understands the true meaning of the sentence. The cleaned text was then converted into mathematical formats using two different methods: Bag-of-Words and TF-IDF. Two statistical classifiers, Naive Bayes and Logistic Regression, were trained and compared to determine which method could best identify human emotion. The results show that Logistic Regression paired with TF-IDF was the most effective approach, achieving an overall accuracy of 69.2\%.
\end{abstract}

\section{Data Preprocessing (Task 1)}
% Explain your cleaning logic and spaCy negation handling.

\section{Feature Representation (Task 2)}
% Discuss BoW vs TF-IDF.

\section{Model Training \& Evaluation (Task 3)}
% Discuss Hyperparameter tuning (C=10.0) and Naive Bayes vs Log Reg.

\section{Error Analysis \& Diagnostics (Task 4)}
% Insert your Confusion Matrix and Feature Importance discussion here.

\section{Conclusion}
% Final reflections.

\end{document}